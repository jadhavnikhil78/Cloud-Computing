Last login: Sun Mar 31 08:13:37 on ttys000
Nikhils-MacBook-Pro:~ dragonheart$ aws s3 cp s3://801075504bucket/SparkLERS.jar .
-bash: aws: command not found
Nikhils-MacBook-Pro:~ dragonheart$ cd Downloads/
Nikhils-MacBook-Pro:Downloads dragonheart$ ssh -i ClouderaKeyPair.pem hadoop@ec2-18-223-209-75.us-east-2.compute.amazonaws.com
Last login: Sun Mar 31 12:14:24 2019 from rrcs-173-95-57-195.midsouth.biz.rr.com

       __|  __|_  )
       _|  (     /   Amazon Linux AMI
      ___|\___|___|

https://aws.amazon.com/amazon-linux-ami/2018.03-release-notes/
20 package(s) needed for security, out of 33 available
Run "sudo yum update" to apply all updates.
                                                                    
EEEEEEEEEEEEEEEEEEEE MMMMMMMM           MMMMMMMM RRRRRRRRRRRRRRR    
E::::::::::::::::::E M:::::::M         M:::::::M R::::::::::::::R   
EE:::::EEEEEEEEE:::E M::::::::M       M::::::::M R:::::RRRRRR:::::R 
  E::::E       EEEEE M:::::::::M     M:::::::::M RR::::R      R::::R
  E::::E             M::::::M:::M   M:::M::::::M   R:::R      R::::R
  E:::::EEEEEEEEEE   M:::::M M:::M M:::M M:::::M   R:::RRRRRR:::::R 
  E::::::::::::::E   M:::::M  M:::M:::M  M:::::M   R:::::::::::RR   
  E:::::EEEEEEEEEE   M:::::M   M:::::M   M:::::M   R:::RRRRRR::::R  
  E::::E             M:::::M    M:::M    M:::::M   R:::R      R::::R
  E::::E       EEEEE M:::::M     MMM     M:::::M   R:::R      R::::R
EE:::::EEEEEEEE::::E M:::::M             M:::::M   R:::R      R::::R
E::::::::::::::::::E M:::::M             M:::::M RR::::R      R::::R
EEEEEEEEEEEEEEEEEEEE MMMMMMM             MMMMMMM RRRRRRR      RRRRRR
                                                                    
[hadoop@ip-172-31-6-253 ~]$ aws s3 cp s3://801075504bucket/SparkLERS.jar .
download: s3://801075504bucket/SparkLERS.jar to ./SparkLERS.jar 
[hadoop@ip-172-31-6-253 ~]$ spark-submit --class "org.ActionRules.Main" --master yarn --deploy-mode client s3://801075504bucket/SparkLERS.jar s3://801075504bucket/attributes.txt s3://801075504bucket/parameters.txt s3://801075504bucket/data.txt s3://801075504bucket/SparkLERSOutput
log4j:ERROR setFile(null,true) call failed.
java.io.FileNotFoundException: /stderr (Permission denied)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:133)
	at org.apache.log4j.FileAppender.setFile(FileAppender.java:294)
	at org.apache.log4j.FileAppender.activateOptions(FileAppender.java:165)
	at org.apache.log4j.DailyRollingFileAppender.activateOptions(DailyRollingFileAppender.java:223)
	at org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:307)
	at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:172)
	at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:104)
	at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:842)
	at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:768)
	at org.apache.log4j.PropertyConfigurator.parseCatsAndRenderers(PropertyConfigurator.java:672)
	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:516)
	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:580)
	at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)
	at org.apache.log4j.LogManager.<clinit>(LogManager.java:127)
	at org.apache.spark.internal.Logging$class.initializeLogging(Logging.scala:120)
	at org.apache.spark.internal.Logging$class.initializeLogIfNecessary(Logging.scala:108)
	at org.apache.spark.deploy.SparkSubmit.initializeLogIfNecessary(SparkSubmit.scala:71)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:79)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:924)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:933)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
log4j:ERROR Either File or DatePattern options are not set for appender [DRFA-stderr].
log4j:ERROR setFile(null,true) call failed.
java.io.FileNotFoundException: /stdout (Permission denied)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:133)
	at org.apache.log4j.FileAppender.setFile(FileAppender.java:294)
	at org.apache.log4j.FileAppender.activateOptions(FileAppender.java:165)
	at org.apache.log4j.DailyRollingFileAppender.activateOptions(DailyRollingFileAppender.java:223)
	at org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:307)
	at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:172)
	at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:104)
	at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:842)
	at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:768)
	at org.apache.log4j.PropertyConfigurator.parseCatsAndRenderers(PropertyConfigurator.java:672)
	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:516)
	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:580)
	at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)
	at org.apache.log4j.LogManager.<clinit>(LogManager.java:127)
	at org.apache.spark.internal.Logging$class.initializeLogging(Logging.scala:120)
	at org.apache.spark.internal.Logging$class.initializeLogIfNecessary(Logging.scala:108)
	at org.apache.spark.deploy.SparkSubmit.initializeLogIfNecessary(SparkSubmit.scala:71)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:79)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:924)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:933)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
log4j:ERROR Either File or DatePattern options are not set for appender [DRFA-stdout].
19/03/31 12:26:36 INFO SparkContext: Running Spark version 2.4.0
19/03/31 12:26:36 INFO SparkContext: Submitted application: SparkAction
19/03/31 12:26:36 INFO SecurityManager: Changing view acls to: hadoop
19/03/31 12:26:36 INFO SecurityManager: Changing modify acls to: hadoop
19/03/31 12:26:36 INFO SecurityManager: Changing view acls groups to: 
19/03/31 12:26:36 INFO SecurityManager: Changing modify acls groups to: 
19/03/31 12:26:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hadoop); groups with view permissions: Set(); users  with modify permissions: Set(hadoop); groups with modify permissions: Set()
19/03/31 12:26:37 INFO Utils: Successfully started service 'sparkDriver' on port 36673.
19/03/31 12:26:37 INFO SparkEnv: Registering MapOutputTracker
19/03/31 12:26:37 INFO SparkEnv: Registering BlockManagerMaster
19/03/31 12:26:37 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
19/03/31 12:26:37 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
19/03/31 12:26:37 INFO DiskBlockManager: Created local directory at /mnt/tmp/blockmgr-5a7e828c-51d7-463a-a961-fcf00d1946fa
19/03/31 12:26:37 INFO MemoryStore: MemoryStore started with capacity 424.4 MB
19/03/31 12:26:37 INFO SparkEnv: Registering OutputCommitCoordinator
19/03/31 12:26:37 INFO Utils: Successfully started service 'SparkUI' on port 4040.
19/03/31 12:26:38 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://ip-172-31-6-253.us-east-2.compute.internal:4040
19/03/31 12:26:38 INFO SparkContext: Added JAR s3://801075504bucket/SparkLERS.jar at s3://801075504bucket/SparkLERS.jar with timestamp 1554035198130
19/03/31 12:26:38 INFO Executor: Starting executor ID driver on host localhost
19/03/31 12:26:38 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38675.
19/03/31 12:26:38 INFO NettyBlockTransferService: Server created on ip-172-31-6-253.us-east-2.compute.internal:38675
19/03/31 12:26:38 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
19/03/31 12:26:38 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ip-172-31-6-253.us-east-2.compute.internal, 38675, None)
19/03/31 12:26:38 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-6-253.us-east-2.compute.internal:38675 with 424.4 MB RAM, BlockManagerId(driver, ip-172-31-6-253.us-east-2.compute.internal, 38675, None)
19/03/31 12:26:38 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ip-172-31-6-253.us-east-2.compute.internal, 38675, None)
19/03/31 12:26:38 INFO BlockManager: external shuffle service port = 7337
19/03/31 12:26:38 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ip-172-31-6-253.us-east-2.compute.internal, 38675, None)
19/03/31 12:26:39 INFO EventLoggingListener: Logging events to hdfs:/var/log/spark/apps/local-1554035198256
19/03/31 12:26:40 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 237.3 KB, free 424.2 MB)
19/03/31 12:26:40 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 24.1 KB, free 424.2 MB)
19/03/31 12:26:40 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ip-172-31-6-253.us-east-2.compute.internal:38675 (size: 24.1 KB, free: 424.4 MB)
19/03/31 12:26:40 INFO SparkContext: Created broadcast 0 from textFile at LERS.scala:26
19/03/31 12:26:40 INFO GPLNativeCodeLoader: Loaded native gpl library
19/03/31 12:26:40 INFO LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev bab859f34a291cb7b3f4e724b59e1b48af69016b]
19/03/31 12:26:40 INFO FileInputFormat: Total input files to process : 1
19/03/31 12:26:40 INFO SparkContext: Starting job: count at LERS.scala:30
19/03/31 12:26:40 INFO DAGScheduler: Got job 0 (count at LERS.scala:30) with 1 output partitions
19/03/31 12:26:40 INFO DAGScheduler: Final stage: ResultStage 0 (count at LERS.scala:30)
19/03/31 12:26:40 INFO DAGScheduler: Parents of final stage: List()
19/03/31 12:26:40 INFO DAGScheduler: Missing parents: List()
19/03/31 12:26:40 INFO DAGScheduler: Submitting ResultStage 0 (s3://801075504bucket/attributes.txt MapPartitionsRDD[1] at textFile at LERS.scala:26), which has no missing parents
19/03/31 12:26:40 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 424.2 MB)
19/03/31 12:26:40 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2037.0 B, free 424.2 MB)
19/03/31 12:26:40 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ip-172-31-6-253.us-east-2.compute.internal:38675 (size: 2037.0 B, free: 424.4 MB)
19/03/31 12:26:40 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1201
19/03/31 12:26:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (s3://801075504bucket/attributes.txt MapPartitionsRDD[1] at textFile at LERS.scala:26) (first 15 tasks are for partitions Vector(0))
19/03/31 12:26:40 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
19/03/31 12:26:40 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 7895 bytes)
19/03/31 12:26:40 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
19/03/31 12:26:40 INFO Executor: Fetching s3://801075504bucket/SparkLERS.jar with timestamp 1554035198130
19/03/31 12:26:40 INFO S3NativeFileSystem: Opening 's3://801075504bucket/SparkLERS.jar' for reading
19/03/31 12:26:40 INFO Utils: Fetching s3://801075504bucket/SparkLERS.jar to /mnt/tmp/spark-e1bf68f5-c241-4ab3-a356-4e7bf08327c9/userFiles-756c1cc2-0ea0-4e64-aefb-65b84c03e301/fetchFileTemp4443283819711102106.tmp
19/03/31 12:26:40 INFO Executor: Adding file:/mnt/tmp/spark-e1bf68f5-c241-4ab3-a356-4e7bf08327c9/userFiles-756c1cc2-0ea0-4e64-aefb-65b84c03e301/SparkLERS.jar to class loader
19/03/31 12:26:41 INFO HadoopRDD: Input split: s3://801075504bucket/attributes.txt:0+54
19/03/31 12:26:41 INFO S3NativeFileSystem: Opening 's3://801075504bucket/attributes.txt' for reading
19/03/31 12:26:41 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 918 bytes result sent to driver
19/03/31 12:26:41 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 328 ms on localhost (executor driver) (1/1)
19/03/31 12:26:41 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
19/03/31 12:26:41 INFO DAGScheduler: ResultStage 0 (count at LERS.scala:30) finished in 0.514 s
19/03/31 12:26:41 INFO DAGScheduler: Job 0 finished: count at LERS.scala:30, took 0.678261 s
19/03/31 12:26:41 INFO SparkContext: Starting job: collect at LERS.scala:35
19/03/31 12:26:41 INFO DAGScheduler: Got job 1 (collect at LERS.scala:35) with 1 output partitions
19/03/31 12:26:41 INFO DAGScheduler: Final stage: ResultStage 1 (collect at LERS.scala:35)
19/03/31 12:26:41 INFO DAGScheduler: Parents of final stage: List()
19/03/31 12:26:41 INFO DAGScheduler: Missing parents: List()
19/03/31 12:26:41 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[2] at map at LERS.scala:33), which has no missing parents
19/03/31 12:26:41 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 3.6 KB, free 424.2 MB)
19/03/31 12:26:41 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.1 KB, free 424.2 MB)
19/03/31 12:26:41 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on ip-172-31-6-253.us-east-2.compute.internal:38675 (size: 2.1 KB, free: 424.4 MB)
19/03/31 12:26:41 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1201
19/03/31 12:26:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[2] at map at LERS.scala:33) (first 15 tasks are for partitions Vector(0))
19/03/31 12:26:41 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
19/03/31 12:26:41 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 7895 bytes)
19/03/31 12:26:41 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
19/03/31 12:26:41 INFO HadoopRDD: Input split: s3://801075504bucket/attributes.txt:0+54
19/03/31 12:26:41 INFO S3NativeFileSystem: Opening 's3://801075504bucket/attributes.txt' for reading
19/03/31 12:26:41 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 857 bytes result sent to driver
19/03/31 12:26:41 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 57 ms on localhost (executor driver) (1/1)
19/03/31 12:26:41 INFO DAGScheduler: ResultStage 1 (collect at LERS.scala:35) finished in 0.074 s
19/03/31 12:26:41 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
19/03/31 12:26:41 INFO DAGScheduler: Job 1 finished: collect at LERS.scala:35, took 0.078500 s
19/03/31 12:26:41 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 464.0 B, free 424.2 MB)
19/03/31 12:26:41 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 149.0 B, free 424.2 MB)
19/03/31 12:26:41 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on ip-172-31-6-253.us-east-2.compute.internal:38675 (size: 149.0 B, free: 424.4 MB)
19/03/31 12:26:41 INFO SparkContext: Created broadcast 3 from broadcast at LERS.scala:43
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 7
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 6
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 11
19/03/31 12:26:41 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 237.3 KB, free 423.9 MB)
19/03/31 12:26:41 INFO BlockManagerInfo: Removed broadcast_2_piece0 on ip-172-31-6-253.us-east-2.compute.internal:38675 in memory (size: 2.1 KB, free: 424.4 MB)
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 1
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 23
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 9
19/03/31 12:26:41 INFO BlockManagerInfo: Removed broadcast_1_piece0 on ip-172-31-6-253.us-east-2.compute.internal:38675 in memory (size: 2037.0 B, free: 424.4 MB)
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 21
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 18
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 2
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 16
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 20
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 17
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 14
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 4
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 8
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 12
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 19
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 3
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 13
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 24
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 10
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 5
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 22
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 15
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 0
19/03/31 12:26:41 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 24.1 KB, free 423.9 MB)
19/03/31 12:26:41 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on ip-172-31-6-253.us-east-2.compute.internal:38675 (size: 24.1 KB, free: 424.4 MB)
19/03/31 12:26:41 INFO SparkContext: Created broadcast 4 from textFile at LERS.scala:52
19/03/31 12:26:41 INFO FileInputFormat: Total input files to process : 1
19/03/31 12:26:41 INFO SparkContext: Starting job: collect at LERS.scala:53
19/03/31 12:26:41 INFO DAGScheduler: Got job 2 (collect at LERS.scala:53) with 1 output partitions
19/03/31 12:26:41 INFO DAGScheduler: Final stage: ResultStage 2 (collect at LERS.scala:53)
19/03/31 12:26:41 INFO DAGScheduler: Parents of final stage: List()
19/03/31 12:26:41 INFO DAGScheduler: Missing parents: List()
19/03/31 12:26:41 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[5] at map at LERS.scala:53), which has no missing parents
19/03/31 12:26:41 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 3.6 KB, free 423.9 MB)
19/03/31 12:26:41 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.1 KB, free 423.9 MB)
19/03/31 12:26:41 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on ip-172-31-6-253.us-east-2.compute.internal:38675 (size: 2.1 KB, free: 424.4 MB)
19/03/31 12:26:41 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1201
19/03/31 12:26:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at map at LERS.scala:53) (first 15 tasks are for partitions Vector(0))
19/03/31 12:26:41 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
19/03/31 12:26:41 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 7895 bytes)
19/03/31 12:26:41 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
19/03/31 12:26:41 INFO HadoopRDD: Input split: s3://801075504bucket/parameters.txt:0+42
19/03/31 12:26:41 INFO S3NativeFileSystem: Opening 's3://801075504bucket/parameters.txt' for reading
19/03/31 12:26:41 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 964 bytes result sent to driver
19/03/31 12:26:41 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 44 ms on localhost (executor driver) (1/1)
19/03/31 12:26:41 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
19/03/31 12:26:41 INFO DAGScheduler: ResultStage 2 (collect at LERS.scala:53) finished in 0.060 s
19/03/31 12:26:41 INFO DAGScheduler: Job 2 finished: collect at LERS.scala:53, took 0.065202 s
4
19/03/31 12:26:41 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 584.0 B, free 423.9 MB)
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 31
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 55
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 70
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 42
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 71
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 25
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 64
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 67
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 47
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 46
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 33
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 65
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 72
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 62
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 32
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 74
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 38
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 56
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 43
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 59
19/03/31 12:26:41 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 184.0 B, free 423.9 MB)
19/03/31 12:26:41 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on ip-172-31-6-253.us-east-2.compute.internal:38675 (size: 184.0 B, free: 424.4 MB)
19/03/31 12:26:41 INFO SparkContext: Created broadcast 6 from broadcast at LERS.scala:57
19/03/31 12:26:41 INFO BlockManagerInfo: Removed broadcast_5_piece0 on ip-172-31-6-253.us-east-2.compute.internal:38675 in memory (size: 2.1 KB, free: 424.4 MB)
19/03/31 12:26:41 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 237.3 KB, free 423.7 MB)
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 49
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 36
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 61
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 52
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 29
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 27
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 44
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 40
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 30
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 54
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 58
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 68
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 28
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 26
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 57
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 41
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 45
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 53
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 37
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 60
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 63
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 51
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 66
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 50
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 34
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 48
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 35
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 73
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 39
19/03/31 12:26:41 INFO ContextCleaner: Cleaned accumulator 69
19/03/31 12:26:41 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 24.1 KB, free 423.7 MB)
19/03/31 12:26:41 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on ip-172-31-6-253.us-east-2.compute.internal:38675 (size: 24.1 KB, free: 424.4 MB)
19/03/31 12:26:41 INFO SparkContext: Created broadcast 7 from textFile at LERS.scala:63
19/03/31 12:26:41 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
19/03/31 12:26:41 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.DirectFileOutputCommitter
19/03/31 12:26:41 INFO DirectFileOutputCommitter: Nothing to setup since the outputs are written directly.
19/03/31 12:26:41 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:78
19/03/31 12:26:41 INFO FileInputFormat: Total input files to process : 1
19/03/31 12:26:41 INFO DAGScheduler: Registering RDD 8 (mapPartitions at LERS.scala:70)
19/03/31 12:26:41 INFO DAGScheduler: Got job 3 (runJob at SparkHadoopWriter.scala:78) with 1 output partitions
19/03/31 12:26:41 INFO DAGScheduler: Final stage: ResultStage 4 (runJob at SparkHadoopWriter.scala:78)
19/03/31 12:26:41 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
19/03/31 12:26:41 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 3)
19/03/31 12:26:41 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[8] at mapPartitions at LERS.scala:70), which has no missing parents
19/03/31 12:26:41 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 6.9 KB, free 423.7 MB)
19/03/31 12:26:41 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 3.7 KB, free 423.6 MB)
19/03/31 12:26:41 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on ip-172-31-6-253.us-east-2.compute.internal:38675 (size: 3.7 KB, free: 424.4 MB)
19/03/31 12:26:41 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1201
19/03/31 12:26:41 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[8] at mapPartitions at LERS.scala:70) (first 15 tasks are for partitions Vector(0))
19/03/31 12:26:41 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
19/03/31 12:26:41 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, ANY, 7878 bytes)
19/03/31 12:26:41 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
19/03/31 12:26:41 INFO HadoopRDD: Input split: s3://801075504bucket/data.txt:0+53593
19/03/31 12:26:41 INFO S3NativeFileSystem: Opening 's3://801075504bucket/data.txt' for reading
19/03/31 12:26:42 INFO MemoryStore: Block rdd_7_0 stored as values in memory (estimated size 174.7 KB, free 423.5 MB)
19/03/31 12:26:42 INFO BlockManagerInfo: Added rdd_7_0 in memory on ip-172-31-6-253.us-east-2.compute.internal:38675 (size: 174.7 KB, free: 424.2 MB)
19/03/31 12:26:45 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1097 bytes result sent to driver
19/03/31 12:26:45 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 3545 ms on localhost (executor driver) (1/1)
19/03/31 12:26:45 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
19/03/31 12:26:45 INFO DAGScheduler: ShuffleMapStage 3 (mapPartitions at LERS.scala:70) finished in 3.578 s
19/03/31 12:26:45 INFO DAGScheduler: looking for newly runnable stages
19/03/31 12:26:45 INFO DAGScheduler: running: Set()
19/03/31 12:26:45 INFO DAGScheduler: waiting: Set(ResultStage 4)
19/03/31 12:26:45 INFO DAGScheduler: failed: Set()
19/03/31 12:26:45 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[11] at saveAsTextFile at LERS.scala:240), which has no missing parents
19/03/31 12:26:45 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 77.3 KB, free 423.4 MB)
19/03/31 12:26:45 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 29.5 KB, free 423.4 MB)
19/03/31 12:26:45 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on ip-172-31-6-253.us-east-2.compute.internal:38675 (size: 29.5 KB, free: 424.2 MB)
19/03/31 12:26:45 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1201
19/03/31 12:26:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[11] at saveAsTextFile at LERS.scala:240) (first 15 tasks are for partitions Vector(0))
19/03/31 12:26:45 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
19/03/31 12:26:45 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, ANY, 7662 bytes)
19/03/31 12:26:45 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
19/03/31 12:26:45 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
19/03/31 12:26:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 30 ms
19/03/31 12:26:45 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.DirectFileOutputCommitter
19/03/31 12:26:45 INFO MultipartUploadOutputStream: close closed:false s3://801075504bucket/SparkLERSOutput/part-00000
19/03/31 12:26:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20190331122641_0011_m_000000_0
19/03/31 12:26:46 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1545 bytes result sent to driver
19/03/31 12:26:46 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 486 ms on localhost (executor driver) (1/1)
19/03/31 12:26:46 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
19/03/31 12:26:46 INFO DAGScheduler: ResultStage 4 (runJob at SparkHadoopWriter.scala:78) finished in 0.528 s
19/03/31 12:26:46 INFO DAGScheduler: Job 3 finished: runJob at SparkHadoopWriter.scala:78, took 4.180923 s
19/03/31 12:26:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
19/03/31 12:26:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: true
19/03/31 12:26:46 INFO DirectFileOutputCommitter: Direct Write: ENABLED
19/03/31 12:26:46 INFO DirectFileOutputCommitter: Nothing to clean up since no temporary files were written.
19/03/31 12:26:46 INFO MultipartUploadOutputStream: close closed:false s3://801075504bucket/SparkLERSOutput/_SUCCESS
19/03/31 12:26:46 INFO SparkHadoopWriter: Job job_20190331122641_0011 committed.
19/03/31 12:26:46 INFO SparkContext: Invoking stop() from shutdown hook
19/03/31 12:26:46 INFO SparkUI: Stopped Spark web UI at http://ip-172-31-6-253.us-east-2.compute.internal:4040
19/03/31 12:26:46 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
19/03/31 12:26:46 INFO MemoryStore: MemoryStore cleared
19/03/31 12:26:46 INFO BlockManager: BlockManager stopped
19/03/31 12:26:46 INFO BlockManagerMaster: BlockManagerMaster stopped
19/03/31 12:26:46 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
19/03/31 12:26:46 INFO SparkContext: Successfully stopped SparkContext
19/03/31 12:26:46 INFO ShutdownHookManager: Shutdown hook called
19/03/31 12:26:46 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-81ceda3f-cc31-451b-aff2-2e7a7a69171e
19/03/31 12:26:46 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-e1bf68f5-c241-4ab3-a356-4e7bf08327c9
[hadoop@ip-172-31-6-253 ~]$ 
